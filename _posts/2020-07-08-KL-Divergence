---
layout: post
title: Kullback-Leibler (KL) Divergence
subtitle: A revisit to KLD
cover-img: /assets/img/2020-07-02-unit-test/summer_ox.jpg
gh-repo: YongchaoHuang/yongchaohuang.github.io
gh-badge: [stats, python]
tags: [algorithm]
comments: true
---

References: <br />
  * [KL Divergence Python Example](https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810).
  * [How to Calculate the KL Divergence for Machine Learning](https://machinelearningmastery.com/divergence-between-probability-distributions/).
  
![Image](https://miro.medium.com/max/480/1*k62ymKzleMFvAQAKR8A_bA.gif)
<div style="text-align: right"> [Images may be subject to copyright] </div>

# Motivation
It was years ago when I first came across this concept. I decided to record it since I have been frequently deriving the entropy relations.


# The concept 
* KLD is a method of measuring statistical distance. Sometimes referred to as “relative entropy.”
* Statistical distance is the general idea of calculating the difference between statistical objects like different probability distributions for a random variable.
* Kullback-Leibler divergence calculates a score that measures the divergence of one probability distribution from another.
* We can think of the KL divergence as distance metric (although it isn’t symmetric) that quantifies the difference between two probability distributions.
* The lower the KL divergence value, the closer the two distributions are to one another.
* The KL divergence is also a key component of Gaussian Mixture Models and t-SNE.
* the KL divergence is not symmetrical. a divergence is a scoring of how one distribution differs from another, where calculating the divergence for distributions P and Q would give a different score from Q and P.
* Divergence scores provide shortcuts for calculating scores such as mutual information (information gain) and cross-entropy used as a loss function for classification models.
* Divergence scores are also used directly as tools for understanding complex modeling problems, such as approximating a target probability distribution when optimizing generative adversarial network (GAN) models.
* Two commonly used divergence scores from information theory are Kullback-Leibler Divergence and Jensen-Shannon Divergence.
* Jensen-Shannon divergence extends KL divergence to calculate a symmetrical score and distance measure of one probability distribution from another.

There are many situations where we may want to compare two probability distributions.e.g., we may have a single random variable and two different probability distributions for the variable, such as a true distribution and an approximation of that distribution.
In situations like this, it can be useful to quantify the difference between the distributions.


For distributions P and Q of a continuous random variable, the Kullback-Leibler divergence is computed as an integral:

![Image](https://miro.medium.com/max/567/1*sl99uqWnmAcZw71aUgcAfQ.png)
<div style="text-align: right"> [Images may be subject to copyright] </div>

if P and Q represent the probability distribution of a discrete random variable, the Kullback-Leibler divergence is calculated as a summation:

![Image](https://miro.medium.com/max/566/1*pqnS3f2aWcKSFqfXxx75JQ.png)
<div style="text-align: right"> [Images may be subject to copyright] </div>

The intuition for the KL divergence score is that when the probability for an event from P is large, but the probability for the same event in Q is small, there is a large divergence. When the probability from P is small and the probability from Q is large, there is also a large divergence, but not as large as the first case.
It is like an expectation of the divergence betweent the true distribution of DGP and the approximate distribution, if you recognise the ratio (also a variable) as a measure of divergence.

The log can be base-2 to give units in “bits,” or the natural logarithm base-e with units in “nats.” When the score is 0, it suggests that both distributions are identical, otherwise the score is positive.
This sum (or integral in the case of continuous random variables) will always be positive, by [the Gibbs inequality](http://en.wikipedia.org/wiki/Gibbs%27_inequality).

Importantly, the KL divergence score is not symmetrical, i.e. KL(P || Q) != KL(Q || P). 

# Application
If we are attempting to approximate an unknown probability distribution, then the target probability distribution from data is P and Q is our approximation of the distribution.
In this case, the KL divergence summarizes the number of additional bits (i.e. calculated with the base-2 logarithm) required to represent an event from the random variable. The better our approximation, the less additional information is required.
'… the KL divergence is the average number of extra bits needed to encode the data, due to the fact that we used distribution q to encode the data instead of the true distribution p.' [Page 58, Machine Learning: A Probabilistic Perspective, 2012.]

# Python Example 1: [KL Divergence Python Example](https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810).
1. import libraries
```
import numpy as np
from scipy.stats import norm
from matplotlib import pyplot as plt
import tensorflow as tf
import seaborn as sns
sns.set()
```
2. define KLD func
```
def kl_divergence(p, q):
    return np.sum(np.where(p != 0, p * np.log(p / q), 0))
```
3. calculate the KLD between two close normal distributions
```
x = np.arange(-10, 10, 0.001)
p = norm.pdf(x, 0, 2)
q = norm.pdf(x, 2, 2)
plt.title('KL(P||Q) = %1.3f' % kl_divergence(p, q))
plt.plot(x, p)
plt.plot(x, q, c='red')
```
4. calculate the KLD between two far away normal distributions
```
q = norm.pdf(x, 5, 4)
plt.title('KL(P||Q) = %1.3f' % kl_divergence(p, q))
plt.plot(x, p)
plt.plot(x, q, c='red')
```

* note that the KL divergence is not symmetrical. i.e. if we swap P and Q, the result is different:
```
plt.title('KL(Q||P) = %1.3f' % kl_divergence(q, p))
plt.plot(x, p)
plt.plot(x, q, c='red')
```

# Python Example 2: [How to Calculate the KL Divergence for Machine Learning](https://machinelearningmastery.com/divergence-between-probability-distributions/).




# Minimizing KLD
The lower the KL divergence, the closer the two distributions are to one another. Therefore, as in the case of t-SNE and Gaussian Mixture Models, we can estimate the Gaussian parameters of one distribution by minimizing its KL divergence with respect to another.


## Methods
Design the procedure to derive and/or select test cases based on an analysis of the internal structure of a component or system.
Two Box Testing methods: White Box Tesing, Black Box Testing

### White Box Testing (also structural testing)
  1. Basics <br />
   * testing based on an analysis of the internal structure of the component or system. <br />
   * the internal structure/design/implementation of the item being tested is known to the tester <br />
   * the tester chooses inputs to exercise paths through the code and determines the appropriate outputs <br />
  The key being, the software program, in the eyes of the tester, is like a white/transparent box; inside which one clearly sees. <br />

  2. Uses <br />
  The testing can be done at system, integration and unit levels of software development. One of the basic goals of whitebox testing is to verify a working flow
  for an application. It involves testing a series of predefined inputs against expected or desired outputs so that when a specific input does not result in the
  expected output, you have encountered a bug. <br />
  
  White Box Testing method mainly applied to Unit Testing, but it is applicable to the following levels of software testing: <br />
   * Unit Testing: For testing paths within a unit.
   * Integration Testing: For testing paths between units.
   * System Testing: For testing paths between subsystems.
  
  What do you verify in White Box Testing? <br />
   White box testing involves the testing of the software code for the following: <br />
   * Internal security holes
   * Broken or poorly structured paths in the coding processes
   * The flow of specific inputs through the code
   * Expected output
   * The functionality of conditional loops
   * Testing of each statement, object, and function on an individual basis
  
  How do you perform White Box Testing? <br />
   two basic steps:  <br />
   * STEP 1) UNDERSTAND THE SOURCE CODE
   * Step 2) CREATE TEST CASES AND EXECUTE
  
  4. White Box Testing Examples <br />
   Consider the following piece of code: <br />
   ```
   Printme (int a, int b) {                       ------------  Printme is a function 
    int result = a+ b; 
    If (result> 0)
    	Print ("Positive", result)
    Else
    	Print ("Negative", result)
    }                                            -----------   End of the source code 
   ```
   The goal of WhiteBox testing in software engineering is to verify all the decision branches, loops, statements in the code. <br />
   To exercise the statements in the above code, WhiteBox test cases would be <br />
     A = 1, B = 1 <br />
     A = -1, B = -3 <br />
  
  5. White Box Testing Techniques <br />
   A major White box testing technique is Code Coverage analysis. Code Coverage analysis eliminates gaps in a Test Case suite. It identifies areas of a program that are not exercised by a set of test cases. Once gaps are identified, you create test cases to verify untested parts of the code, thereby increasing the quality of the software product. <br />
   
   There are automated tools available to perform Code coverage analysis. Below are a few coverage analysis techniques a box tester can use: <br />
    * Statement Coverage:- This technique requires every possible statement in the code to be tested at least once during the testing process of software engineering. <br />
    * Branch Coverage - This technique checks every possible path (if-else and other conditional loops) of a software application. <br />
    * Apart from above, there are numerous coverage types such as Condition Coverage, Multiple Condition Coverage, Path Coverage, Function Coverage etc. Each technique has its own merits and attempts to test (cover) all parts of software code. Using Statement and Branch coverage you generally attain 80-90% code coverage which is sufficient. <br />
    
   White Box Testing Tools <br />
    * Parasoft Jtest
    * EclEmma
    * NUnit
    * PyUnit
    * HTMLUnit
    * CppUnit


  6. Advantages <br />
   * Testing can be commenced at an earlier stage. One need not wait for the GUI to be available.
   * Testing is more thorough, with the possibility of covering most paths
   * Code optimization by finding hidden errors.
   * White box tests cases can be easily automated.
   * Testing is more thorough as all code paths are usually covered.
   * Testing can start early in SDLC even if GUI is not available.
  
  7. Disadvantages <br />
   * Since tests can be very complex, highly skilled resources are required, with a thorough knowledge of programming and implementation.
   * Test script maintenance can be a burden if the implementation changes too frequently.
   * Since this method of testing is closely tied to the application being tested, tools to cater to every kind of implementation/platform may not be readily available.
   * White box testing can be quite complex and expensive.
   * White box testing requires professional resources, with a detailed understanding of programming and implementation.
   * White box testing is time-consuming, bigger programming applications take the time to test fully.
 
  8. Other notes <br />
   White box testing can be quite complex. The complexity involved has a lot to do with the application being tested. A small application that performs a single simple operation could be white box tested in few minutes, while larger programming applications take days, weeks and even longer to fully test


### Black Box Testing (also known as Behavioral Testing)
![Image](http://34.94.91.25/wp-content/uploads/2010/12/black_box_testing-300x149.gif)
<div style="text-align: left"> [Images may be subject to copyright] </div>
  1. Basics <br />
  Testing, either functional or non-functional, without reference to the internal structure of the component or system. <br />
   * internal structure/design/implementation of the item being tested is not known to the tester
   * can be functional or non-functional, though usually functional.
  The key being, the software program, in the eyes of the tester, is like a black box; inside which one cannot see. <br />
  
  This method attempts to find errors in the following categories: <br />
   * Incorrect or missing functions
   * Interface errors
   * Errors in data structures or external database access
   * Behavior or performance errors
   * Initialization and termination errors

  2. Uses <br />
  Design a procedure to derive and/or select test cases based on an analysis of the specification, either functional or non-functional, of a component or system without reference to its internal structure. <br />
  Black Box Testing method is applicable to the following levels of software testing: <br />
   * Integration Testing
   * System Testing
   * Acceptance Testing
   The higher the level, and hence the bigger and more complex the box, the more black-box testing method comes into use. <br />
   
  3. Black Box Testing Techniques <br />
   * Equivalence Partitioning: It is a software test design technique that involves dividing input values into valid and invalid partitions and selecting representative values from each partition as test data.
   * Boundary Value Analysis: It is a software test design technique that involves the determination of boundaries for input values and selecting values that are at the boundaries and just inside/ outside of the boundaries as test data.
   * Cause-Effect Graphing: It is a software test design technique that involves identifying the cases (input conditions) and effects (output conditions), producing a Cause-Effect Graph, and generating test cases accordingly.

  4. Advantages <br />
   * Tests are done from a user’s point of view and will help in exposing discrepancies in the specifications.
   * Tester need not know programming languages or how the software has been implemented.
   * Tests can be conducted by a body independent from the developers, allowing for an objective perspective and the avoidance of developer-bias.
   * Test cases can be designed as soon as the specifications are complete.
  
  5. Disadvantages <br />
   * Only a small number of possible inputs can be tested and many program paths will be left untested.
   * Without clear specifications, which is the situation in many projects, test cases will be difficult to design.
   * Tests can be redundant if the software designer/developer has already run a test case.
   * Ever wondered why a soothsayer closes the eyes when foretelling events? So is almost the case in Black Box Testing.
 

### Differences Between Black Box Testing and White Box Testing

![Image](https://github.com/YongchaoHuang/yongchaohuang.github.io/blob/master/assets/img/2020-07-02-unit-test/difference.png?raw=true)
<div style="text-align: right"> [Source:http://softwaretestingfundamentals.com/differences-between-black-box-testing-and-white-box-testing/] </div>

For a combination of the two testing methods, see [Gray Box Testing](http://softwaretestingfundamentals.com/gray-box-testing/).
  
  
  
   
